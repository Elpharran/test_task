# Система обратного поиска (reverse text search) для кратких описаний статей с ресурса Gazeta.ru. 

[Ссылка на решение, прототип веб-сервиса *временно отсутствует*]()

**Задание**

*Вам необходимо для кратких описаний (поле summary) получить векторное представление (эмбеддинг) с помощью предобученного нейросетевого энкодера и для произвольного текстового запроса также рассчитать эмбеддинг и найти топ-10 схожих описаний по убыванию косинусной близости (cosine similarity) эмбеддингов датасета.*
*В качестве энкодера предлагается взять модель paraphrase-multilingual-mpnet-base-v2, но можно выбрать любой другой на основе архитектуры трансформер.*

**Комментарии к реализации**

Поскольку нам предстоит искать схожие описания, перед нами задача симметричного семантического поиска, под которую в библиотеке Sentence-transformers подходят несколько моделей. Однако, не все из них поддерживают работу с текстом на русском языке, поэтому из доступных был выбран именно рекомендованный энкодер — paraphrase-multilingual-mpnet-base-v2, который обладет наибольшей эффективностью.

Для обработки всех данных понадобилось около 3-х часов, полученные векторные представления сохранены в файле `embeddings.npy` для дальнейшей работы.

Топ релевантных результатов по запросу формируется на основании максимальной косинусной близости. Метод можно было бы реализовать как вручную с использованием библиотек scikit-learn, так и при помощи функции [util.semantic_search](https://www.sbert.net/examples/applications/semantic-search/README.html#util-semantic-search), но для ускорения поиска мы обратились к FAISS.

В этой библиотеке нет готового метода расчёта косинусной близости, но есть возможность проиндексировать векторы с вычислением внутреннего произведения IndexFlatIP, а затем воспользоваться тем фактом, что косинусная близость — это скалярное произведение нормализованных векторов, [источник](https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances#metric_inner_product).

Данных у нас не так много, но разница оказалась заметна даже на таком количестве объектов — по итогу с помощью FAISS поиск получился в два раза быстрее, при этом качество осталось неизменным, поскольку благодаря объёму данных можно было спокойно использовать полный индекс.

- `data_prep.py` — скрипт для подготовки данных,
- `sample_results.txt` — результат работы пяти тестовых запросов.



